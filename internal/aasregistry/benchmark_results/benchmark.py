import subprocess
import time
import json
import os
import requests
import uuid
import copy
import random
import base64
from datetime import datetime
import signal
import sys

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMPOSE_FILE = "docker_compose/docker_compose.yml"
DB_CONTAINER = "postgres_db"  # optional: used if you have a healthcheck on DB
DISCOVERY_URL = "http://localhost:5004/shell-descriptors"

JSON_FILE = "bodies/complex.json"  # path to your file
TOTAL_ITERS = 10000
SEED = 42

# How many iterations should be POST-only (prewarm). Set to 0 to disable.
PREWARM_ITERS = 1000

OP_WEIGHTS = {
    "post": 0.4,
    "get": 0.2,
    "search_all": 0.2,
    "search_limit100": 0.2
}

LOG_FILE = "runtime_results.json"

# When True, each log entry includes request_url, request_body, and response_body
LOG_REQUEST_DETAILS = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run(cmd: str):
    print(f"â–¶ {cmd}")
    return subprocess.run(cmd, shell=True, check=False)

def wait_for_container_health(container: str, timeout: int = 120):
    """
    Waits for a container to be healthy via 'podman inspect'.
    Only useful if the container defines a HEALTHCHECK in your compose.
    """
    print(f"â³ Waiting for '{container}' to be healthy...")
    start = time.time()
    while time.time() - start < timeout:
        result = subprocess.run(
            f"podman inspect --format='{{{{json .State.Health}}}}' {container}",
            shell=True, capture_output=True, text=True
        )
        if '"Status":"healthy"' in (result.stdout or ""):
            print(f"âœ… {container} is healthy")
            return True
        time.sleep(3)
    print(f"âŒ Timeout waiting for '{container}' health.")
    return False

def wait_for_http(url: str, timeout: int = 180):
    print(f"â³ Waiting for service at {url}")
    start = time.time()
    while time.time() - start < timeout:
        try:
            r = requests.get(url, timeout=2)
            if r.status_code < 500:
                print(f"âœ… Service responding: HTTP {r.status_code}")
                return True
        except Exception:
            pass
        time.sleep(2)
    print("âŒ Timeout waiting for HTTP readiness.")
    return False

def url_b64_encode(s: str):
    return base64.urlsafe_b64encode(s.encode()).decode().rstrip("=")

def ts():
    return datetime.now().strftime("%H:%M:%S")



# --- Load JSON file ---
with open(JSON_FILE, "r", encoding="utf-8") as f:
    payload_template = json.load(f)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cleanup handler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cleanup():
    print("\nğŸ§¹ Stopping Docker stack...")
    run(f"podman compose -f {COMPOSE_FILE} down -v")

def sig_handler(sig, frame):
    print("\nâš  Interrupted by user.")
    cleanup()
    sys.exit(0)

signal.signal(signal.SIGINT, sig_handler)
signal.signal(signal.SIGTERM, sig_handler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸš€ Starting Docker Compose...")
    up_rc = run(f"podman compose -f {COMPOSE_FILE} up -d --build").returncode
    if up_rc != 0:
        print("âŒ Failed to start the compose stack (check build paths and context).")
        sys.exit(1)

    # If you want to wait for DB health (only meaningful if healthcheck is defined on DB):
    # wait_for_container_health(DB_CONTAINER)  # uncomment if desired

    if not wait_for_http(DISCOVERY_URL):
        cleanup()
        return

    # Normalize prewarm iterations if they exceed total
    effective_prewarm = max(0, min(PREWARM_ITERS, TOTAL_ITERS))

    print("ğŸ”¥ Starting benchmark...")
    if effective_prewarm > 0:
        print(f"â™¨ï¸  Prewarm enabled: first {effective_prewarm} ops will be POST only.")

    session = requests.Session()
    rng = random.Random(SEED)

    logs = []
    descriptor_pool = []  # store descriptor ids (payload["id"]) to use for GET
    submodel_pool = []    # optional storage of submodel ids (not used now)

    ops = list(OP_WEIGHTS.keys())
    weights = list(OP_WEIGHTS.values())

    for i in range(TOTAL_ITERS):
        # Force POST during prewarm phase
        if i < effective_prewarm:
            op = "post"
        else:
            op = rng.choices(ops, weights)[0]
            # If GET chosen but we have nothing to GET yet, fall back to searches
            if op == "get" and not descriptor_pool:
                op = rng.choice(["search_all", "search_limit100"])

        req_url = None
        req_body = None
        resp_body = None

        t0 = time.perf_counter()
        ok = False
        code = None

        try:
            if op == "post":
                payload = copy.deepcopy(payload_template)

                # Unique descriptor ID (AAS descriptor id)
                descriptor_id = f"https://example.org/aas/{uuid.uuid4()}"
                payload["id"] = descriptor_id

                # Unique submodel descriptor IDs
                if "submodelDescriptors" in payload and payload["submodelDescriptors"]:
                    for sm in payload["submodelDescriptors"]:
                        sm["id"] = f"https://example.org/submodel/{uuid.uuid4()}"

                req_url = DISCOVERY_URL
                req_body = payload

                r = session.post(req_url, json=payload)
                ok = r.ok
                code = r.status_code
                if LOG_REQUEST_DETAILS:
                    try:
                        resp_body = r.json()
                    except ValueError:
                        resp_body = r.text

                if ok:
                    descriptor_pool.append(descriptor_id)
                    for sm in payload.get("submodelDescriptors", []):
                        submodel_pool.append(sm["id"])

            elif op == "get":
                # Use the stored descriptor id for GET-by-id, base64url-encoded
                descriptor_id = rng.choice(descriptor_pool)
                encoded = url_b64_encode(descriptor_id)
                req_url = f"{DISCOVERY_URL}/{encoded}"

                r = session.get(req_url)
                ok = r.ok
                code = r.status_code
                if LOG_REQUEST_DETAILS:
                    try:
                        resp_body = r.json()
                    except ValueError:
                        resp_body = r.text

            elif op == "search_all":
                req_url = DISCOVERY_URL

                r = session.get(req_url)
                ok = r.ok
                code = r.status_code
                if LOG_REQUEST_DETAILS:
                    try:
                        resp_body = r.json()
                    except ValueError:
                        resp_body = r.text

            elif op == "search_limit100":
                # Choose a random AAS identifier for the cursor
                if descriptor_pool:
                    cursor_id = rng.choice(descriptor_pool)
                else:
                    # Fallback if nothing has been posted yet
                    cursor_id = f"https://example.org/aas/{uuid.uuid4()}"

                # Use requests params so URL is encoded correctly
                params = {"limit": 100, "cursor": cursor_id}
                r = session.get(DISCOVERY_URL, params=params)

                # For logging (captures the fully encoded URL)
                req_url = r.request.url

                ok = r.ok
                code = r.status_code
                if LOG_REQUEST_DETAILS:
                    try:
                        resp_body = r.json()
                    except ValueError:
                        resp_body = r.text


        except Exception as e:
            print(f"[{ts()}] âŒ Error: {e}")

        t1 = time.perf_counter()
        entry = {
            "iter": i,
            "op": op,
            "code": code,
            "ok": ok,
            "duration_ms": int((t1 - t0) * 1000)
        }

        if LOG_REQUEST_DETAILS:
            entry["request_url"] = req_url
            entry["request_body"] = req_body
            entry["response_body"] = resp_body

        logs.append(entry)

        if i % 100 == 0:
            print(f"[{ts()}] {i}/{TOTAL_ITERS} ops done | descriptors={len(descriptor_pool)} | submodels={len(submodel_pool)}")

    print("âœ… Benchmark finished.")
    print(f"ğŸ’¾ Saving results â†’ {LOG_FILE}")
    with open(LOG_FILE, "w") as f:
        json.dump(logs, f, indent=2)

    cleanup()
    print("ğŸ Done!")


if __name__ == "__main__":
    main()
